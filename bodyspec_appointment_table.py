# -*- coding: utf-8 -*-
"""bodyspec_appointment_tables_script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1884sfQZOzadHnh0NHTf8tARhf2-fm9EY
"""

# Setup
import requests
import os
import datetime
import pandas as pd
from pytz import timezone
from zoneinfo import ZoneInfo
import numpy as np

# === Constants ===
URL = "https://www.bodyspec.com/graphql"
HEADERS = {
    "Content-Type": "application/json",
    "User-Agent": "Mozilla/5.0"
}
REGIONS = ["austin", "dallas", "norcal", "seattle", "socal"]
start_date = datetime.date.today().isoformat()

# === Collect all appointments here ===
all_rows = []
scrape_time = datetime.datetime.utcnow().isoformat()

# === Loop over regions ===
for region in REGIONS:
    print(f"\nüîç Fetching region: {region}")
    cursor = None

    while True:
        # Construct GraphQL payload with pagination
        payload = {
            "operationName": "Events",
            "variables": {
                "filter": {
                    "region": region,
                    "startDate": start_date
                },
                "page": {
                    "first": 50
                }
            },
            "query": """
            query Events($filter: EventsInput, $page: PageInput) {
              events(filter: $filter, page: $page) {
                edges {
                  node {
                    appts {
                      id
                      canReserve
                      details {
                        start {
                          unix
                          timezone
                          time(fmt: "h:mm a")
                        }
                      }
                    }
                    location {
                      details {
                        slug
                        isStore
                      }
                      regions {
                        primary
                      }
                    }
                  }
                }
                pageInfo {
                  lastCursor
                  hasNextPage
                }
              }
            }
            """
        }

        if cursor:
            payload["variables"]["page"]["after"] = cursor

        response = requests.post(URL, headers=HEADERS, json=payload)
        data = response.json()

        # === Handle errors or missing data ===
        if "errors" in data:
            print(f"‚ùå GraphQL error for region '{region}':")
            for err in data["errors"]:
                print(" ‚Üí", err["message"])
            break  # Skip this region

        if "data" not in data or "events" not in data["data"]:
            print(f"‚ö†Ô∏è  No event data found for region '{region}'")
            break

        # === Parse appointment rows ===
        events = data["data"]["events"]["edges"]
        for edge in events:
            node = edge["node"]
            loc = node["location"]
            #lat = loc["details"]["coords"]["lat"]
            #lng = loc["details"]["coords"]["lng"]
            slug = loc["details"]["slug"]
            is_store = loc["details"]["isStore"]
            region_name = loc["regions"]["primary"]

            for appt in node["appts"]:
                appt_details = appt["details"]["start"]
                all_rows.append({
                    "appointment_id": appt["id"],
                    "location": slug,
                    #"latitude": lat,
                    #"longitude": lng,
                    "region": region_name,
                    "is_store": is_store,
                    "unix": appt_details["unix"],
                    "timezone": appt_details["timezone"],
                    "can_reserve": appt["canReserve"],
                    "scraped_at": scrape_time
                })

        # === Pagination handling ===
        page_info = data["data"]["events"]["pageInfo"]
        if not page_info["hasNextPage"]:
            break
        cursor = page_info["lastCursor"]

# === Build DataFrame ===
df = pd.DataFrame(all_rows)

# === Convert UNIX time to local ===
def convert_to_appointment_time(row):
    # Convert UNIX timestamp to timezone-aware datetime
    tz = ZoneInfo(row["timezone"])
    dt = datetime.datetime.fromtimestamp(int(row["unix"]), tz=tz)
    return dt.isoformat()

df["appointment_time"] = df.apply(convert_to_appointment_time, axis=1)

# === Final output ===
print("\n‚úÖ Example results:")
print(df.head())
print(f"\nüì¶ Total appointments pulled: {len(df)}")
print(f"üìç Unique locations: {df['location'].nunique()}")

print(df.dtypes)

# === Convert data to what Supabase is expecting === #
# Convert datetime columns to ISO 8601 strings
df["appointment_time"] = pd.to_datetime(df["appointment_time"], errors="coerce")
df["scraped_at"] = pd.to_datetime(df["scraped_at"], errors="coerce")

df["appointment_time"] = df["appointment_time"].apply(lambda x: x.isoformat() if pd.notnull(x) else None)
df["scraped_at"] = df["scraped_at"].apply(lambda x: x.isoformat() if pd.notnull(x) else None)

# Replace any NaNs with None
df = df.replace({np.nan: None})

# Convert to list of dicts
records = df[[
    "appointment_id", "location", "region", "is_store",
    "can_reserve", "appointment_time", "scraped_at"
]].to_dict(orient="records")

# === Insert into Supabase ===
from supabase import create_client, Client

url = os.environ["SUPABASE_URL"]
key = os.environ["SUPABASE_KEY"]
supabase: Client = create_client(url, key)

# Ensure we have the right environment variables
if not url:
    print("‚ùó SUPABASE_URL is not set or is empty.")
else:
    print(f"üîç SUPABASE_URL is set (length: {len(url)})")

if not key:
    print("‚ùó SUPABASE_KEY is not set or is empty.")
else:
    print(f"üîç SUPABASE_KEY is set (length: {len(key)})")

# Test Supabase URL to see if it works
if not url:
    print("‚ùó Supabase URL environment variable is not set.")
else:
    print(f"üîç Testing Supabase URL: {url}")
    try:
        response = requests.get(url, timeout=10)
        print(f"‚úÖ GET {url} returned status code: {response.status_code}")
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Failed to reach Supabase URL: {e}")

# Try pushing results to Supabase
try:
    response = supabase.table("Appointments").insert(records).execute()
    print("‚úÖ Successfully inserted records.")
    print("üì¶ Upload response:", response)
except Exception as e:
    print("‚ùå Failed to insert data into Supabase:", e)


# Check result
print("\nüì§ Upload response:")
print(response)
