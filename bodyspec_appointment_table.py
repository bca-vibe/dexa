# -*- coding: utf-8 -*-
"""bodyspec_appointment_tables_script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1884sfQZOzadHnh0NHTf8tARhf2-fm9EY
"""

# Setup
import requests
import datetime
import pandas as pd
from pytz import timezone
from zoneinfo import ZoneInfo

# === Constants ===
URL = "https://www.bodyspec.com/graphql"
HEADERS = {
    "Content-Type": "application/json",
    "User-Agent": "Mozilla/5.0"
}
REGIONS = ["austin", "dallas", "norcal", "seattle", "socal"]
start_date = datetime.date.today().isoformat()

# === Collect all appointments here ===
all_rows = []
scrape_time = datetime.utcnow().isoformat()

# === Loop over regions ===
for region in REGIONS:
    print(f"\n🔍 Fetching region: {region}")
    cursor = None

    while True:
        # Construct GraphQL payload with pagination
        payload = {
            "operationName": "Events",
            "variables": {
                "filter": {
                    "region": region,
                    "startDate": start_date
                },
                "page": {
                    "first": 50
                }
            },
            "query": """
            query Events($filter: EventsInput, $page: PageInput) {
              events(filter: $filter, page: $page) {
                edges {
                  node {
                    appts {
                      id
                      canReserve
                      details {
                        start {
                          unix
                          timezone
                          time(fmt: "h:mm a")
                        }
                      }
                    }
                    location {
                      details {
                        slug
                        isStore
                      }
                      regions {
                        primary
                      }
                    }
                  }
                }
                pageInfo {
                  lastCursor
                  hasNextPage
                }
              }
            }
            """
        }

        if cursor:
            payload["variables"]["page"]["after"] = cursor

        response = requests.post(URL, headers=HEADERS, json=payload)
        data = response.json()

        # === Handle errors or missing data ===
        if "errors" in data:
            print(f"❌ GraphQL error for region '{region}':")
            for err in data["errors"]:
                print(" →", err["message"])
            break  # Skip this region

        if "data" not in data or "events" not in data["data"]:
            print(f"⚠️  No event data found for region '{region}'")
            break

        # === Parse appointment rows ===
        events = data["data"]["events"]["edges"]
        for edge in events:
            node = edge["node"]
            loc = node["location"]
            lat = loc["details"]["coords"]["lat"]
            lng = loc["details"]["coords"]["lng"]
            slug = loc["details"]["slug"]
            is_store = loc["details"]["isStore"]
            region_name = loc["regions"]["primary"]

            for appt in node["appts"]:
                appt_details = appt["details"]["start"]
                all_rows.append({
                    "appointment_id": appt["id"],
                    "location": slug,
                    "latitude": lat,
                    "longitude": lng,
                    "region": region_name,
                    "is_store": is_store,
                    "unix": appt_details["unix"],
                    "timezone": appt_details["timezone"],
                    "can_reserve": appt["canReserve"],
                    "scraped_at": scrape_time
                })

        # === Pagination handling ===
        page_info = data["data"]["events"]["pageInfo"]
        if not page_info["hasNextPage"]:
            break
        cursor = page_info["lastCursor"]

# === Build DataFrame ===
df = pd.DataFrame(all_rows)

# === Convert UNIX time to local ===
def convert_to_appointment_time(row):
    # Convert UNIX timestamp to timezone-aware datetime
    tz = ZoneInfo(row["timezone"])
    dt = datetime.datetime.fromtimestamp(int(row["unix"]), tz=tz)
    return dt.isoformat()

df["appointment_time"] = df.apply(convert_to_appointment_time, axis=1)

# === Drop any rows missing required data before sending to Supabase === #
required_cols = ["appointment_id", "location", "region", "is_store", "can_reserve", "appointment_time", "latitude", "longitude", "scraped_at"]
df = df.dropna(subset=required_cols)

# === Insert into Supabase ===
from supabase import create_client, Client
import os

url = os.environ["SUPABASE_URL"]
key = os.environ["SUPABASE_KEY"]
supabase: Client = create_client(url, key)


# Choose columns to insert
records = df[[
    "appointment_id", "location", "region", "is_store",
    "can_reserve", "appointment_time", "latitude", "longitude", "scraped_at"
]].to_dict(orient="records")

try:
    response = supabase.table("appointments").insert(records).execute()

    if response.get("status_code", 200) >= 300:
        print("⚠️ Error inserting records:", response)
    else:
        print("✅ Successfully inserted records.")
except Exception as e:
    print("❌ Failed to insert data into Supabase:", e)


# Check result
print("\n📤 Upload response:")
print(response)